{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": "# 梯度下降\n\n## 梯度的定义\n\n$\\nabla f \u003d \\left(\\frac{\\partial f}{\\partial x_1}; \\frac{\\partial f}{\\partial x_2};...;\\frac{\\partial f}{\\partial x_n} \\right)$\n\n$\\theta_{t+1} \u003d \\theta_t - \\alpha_t \\nabla f(\\theta_t)$\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "source": "import tensorflow as tf\nfrom tensorflow import keras\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "source": "w \u003d tf.constant(1.)\nx \u003d tf.constant(2.)\ny \u003d x * w\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "source": "with tf.GradientTape() as tape:\n    tape.watch([w])\n    y2 \u003d x * w\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "data": {
            "text/plain": "[\u003ctf.Tensor: id\u003d45, shape\u003d(), dtype\u003dfloat32, numpy\u003d2.0\u003e]"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 7
        }
      ],
      "source": "# 只能执行一次\ntape.gradient(y2, [w])\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# 可以求导多次\nwith tf.GradientTape(persistent\u003dTrue) as tape:\n    tape.watch([w])\n    y2 \u003d x * w\n\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "dy_dw:  tf.Tensor(3.0, shape\u003d(), dtype\u003dfloat32)\ndy_db:  tf.Tensor(1.0, shape\u003d(), dtype\u003dfloat32)\nd2y_dw2:  None\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# 二阶求导\nw \u003d tf.Variable(1.0) # 默认watch\nb \u003d tf.Variable(2.0)\nx \u003d tf.Variable(3.0)\n\nwith tf.GradientTape() as tape1:\n    with tf.GradientTape() as tape2:\n        y \u003d x * w + b\n    dy_dw, dy_db \u003d tape2.gradient(y, [w, b])\nd2y_dw2 \u003d tape1.gradient(dy_dw, w)\n\nprint(\"dy_dw: \", dy_dw)\nprint(\"dy_db: \", dy_db)\nprint(\"d2y_dw2: \", d2y_dw2)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## 激活函数的梯度\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "a \u003d tf.linspace(-10., 10, 10)\n\nwith tf.GradientTape() as tape:\n    tape.watch(a)\n    y \u003d tf.sigmoid(a)\n\ngrads \u003d tape.gradient(y, [a])\n\nprint(\"A: \", a)\nprint(\"y: \", y)\nprint(\"Gradient of a: \", grads)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "A:  tf.Tensor(\n[-10.         -7.7777777  -5.5555553  -3.333333   -1.1111107   1.1111116\n   3.333334    5.5555563   7.7777786  10.       ], shape\u003d(10,), dtype\u003dfloat32)\ny:  tf.Tensor(\n[4.5388937e-05 4.1878223e-04 3.8510561e-03 3.4445226e-02 2.4766389e-01\n 7.5233626e-01 9.6555483e-01 9.9614894e-01 9.9958128e-01 9.9995458e-01], shape\u003d(10,), dtype\u003dfloat32)\nGradient of a:  [\u003ctf.Tensor: id\u003d189, shape\u003d(10,), dtype\u003dfloat32, numpy\u003d\narray([4.5386874e-05, 4.1860685e-04, 3.8362255e-03, 3.3258751e-02,\n       1.8632649e-01, 1.8632641e-01, 3.3258699e-02, 3.8362255e-03,\n       4.1854731e-04, 4.5416677e-05], dtype\u003dfloat32)\u003e]\n"
          ],
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## 损失函数梯度\n\n$\\frac{\\partial p_i}{\\partial a_j} \u003d \\frac{\\partial \\frac{e^{a_i}}{\\sum^N_{k\u003d1} e^{a_k}}}{\\partial a_j}$\n\n- i\u003dj: $\\frac{\\partial p_i}{\\partial a_j} \u003d p_j(1 - p_j)$\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "w gradients:  tf.Tensor(\n[[-0.00381645  0.12483726 -0.12102084]\n [ 0.00143867 -0.0862134   0.08477473]\n [ 0.0003233  -0.04569282  0.04536951]\n [-0.00371393  0.1185912  -0.11487729]], shape\u003d(4, 3), dtype\u003dfloat32)\nb gradients:  tf.Tensor([-0.00330473  0.12689719 -0.12359247], shape\u003d(3,), dtype\u003dfloat32)\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "x \u003d tf.random.normal([2, 4])\nw \u003d tf.random.normal([4, 3])\nb \u003d tf.zeros([3])\ny \u003d tf.constant([2, 0])\n\nwith tf.GradientTape() as tape:\n    tape.watch([w, b])\n    prob \u003d tf.nn.softmax(x@w + b, axis\u003d1)\n    loss \u003d tf.reduce_mean(tf.losses.MSE(tf.one_hot(y, depth\u003d3), prob))\n\ngrads \u003d tape.gradient(loss, [w, b])\nprint(\"w gradients: \", grads[0])\nprint(\"b gradients: \", grads[1])\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "w gradients:  tf.Tensor(\n[[-1.0553846e-01  9.4311126e-02  1.1227328e-02]\n [ 3.3583328e-01 -3.0006656e-01 -3.5766724e-02]\n [-2.4949678e-03  2.6152630e-03 -1.2028698e-04]\n [ 4.9271515e-01 -4.3982694e-01 -5.2888218e-02]], shape\u003d(4, 3), dtype\u003dfloat32)\nb gradients:  tf.Tensor([-0.46352893  0.4142054   0.04932352], shape\u003d(3,), dtype\u003dfloat32)\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "x \u003d tf.random.normal([2, 4])\nw \u003d tf.random.normal([4, 3])\nb \u003d tf.zeros([3])\ny \u003d tf.constant([2, 0])\n\nwith tf.GradientTape() as tape:\n    tape.watch([w, b])\n    logits \u003d x@w + b\n    loss \u003d tf.reduce_mean(tf.losses.categorical_crossentropy(tf.one_hot(y, depth\u003d3), logits, from_logits\u003dTrue))\n\ngrads \u003d tape.gradient(loss, [w, b])\nprint(\"w gradients: \", grads[0])\nprint(\"b gradients: \", grads[1])\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\n\n\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}